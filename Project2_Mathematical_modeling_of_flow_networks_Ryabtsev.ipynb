{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2_Mathematical_modeling_of_flow_networks_Ryabtsev.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "UnXSCkao71sI",
        "r5YChTrhd57A",
        "yks-HiNouBEs",
        "OAf6BU6_aXx-",
        "ktRf0kQYuSkx",
        "3h_trJ9NWF5m",
        "b-RJEgr6WLPG",
        "YkRFl1h9Wdxp"
      ],
      "authorship_tag": "ABX9TyNNBxJHSU3V9zNzwiqJyFu1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anryabtsev/Flow_nets/blob/main/Project2_Mathematical_modeling_of_flow_networks_Ryabtsev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Study of the rate of convergence of methods on average for the choice of the starting point.**\n",
        "\n",
        "### **Experiment design:** we choose several  𝑅 , for each unique  𝑅  we generate random points from the sphere with radius  𝑅  and center in  𝑡∗ . Run experiments with different starting points and collect histories (trajectories) for certain number of iterations (equal for each method). Look at duality gap reduction in average."
      ],
      "metadata": {
        "id": "8cTpGDN_DMj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages installation"
      ],
      "metadata": {
        "id": "UnXSCkao71sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanf"
      ],
      "metadata": {
        "id": "FIfgeG1_77F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz9Xz6Jo7vgi"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://downloads.skewed.de/apt bionic main\" >> /etc/apt/sources.list\n",
        "!apt-key adv --keyserver keys.openpgp.org --recv-key 612DEFB798507F25\n",
        "!apt-get update\n",
        "!apt-get install python3-graph-tool python3-cairo python3-matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "r5YChTrhd57A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GHxSVgJf9dat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/TransportNet\")"
      ],
      "metadata": {
        "id": "7ZDnDNH49ddn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import data_handler as dh\n",
        "import model as md\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from scipy.stats import bernoulli"
      ],
      "metadata": {
        "id": "qpc-I4_Q9df_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "numba.__version__"
      ],
      "metadata": {
        "id": "LTg2cVTF9hZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "print(sys.version)\n",
        "print(sys.version_info)\n",
        "from platform import python_version\n",
        "print('python', python_version())\n",
        "print('numpy', np.__version__)\n",
        "print('pandas', pd.__version__)\n",
        "import graph_tool\n",
        "print('graph_tool', graph_tool.__version__)"
      ],
      "metadata": {
        "id": "TAmOiio-9hbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start"
      ],
      "metadata": {
        "id": "dCO1t9L89rG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting functions"
      ],
      "metadata": {
        "id": "p_TP4jGzt8yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cmap(n, name='tab20'):\n",
        "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
        "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
        "    return plt.cm.get_cmap(name, n)\n",
        "\n",
        "\n",
        "def plot1(means, stds):\n",
        "    \"\"\"\n",
        "    input: res[\"final_dg_means\"], res[\"final_dg_stds\"]\n",
        "    \"\"\"\n",
        "    length = len(means)\n",
        "    x_vals = np.linspace(1, length, length)\n",
        "\n",
        "    plt.figure(figsize = (12, 6))\n",
        "    plt.errorbar(x_vals, means, stds, linestyle='None', marker='^', label=\"Average duality gap with standard deviation\")\n",
        "\n",
        "    plt.legend(loc='best', fontsize=17)\n",
        "    plt.title(f\"Dependence of final duality gap on R for 5000 iterations\", fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot2(histories, normalized=False):\n",
        "    \"\"\"\n",
        "    input: res[\"dg_histories\"]\n",
        "    \"\"\"\n",
        "    first_key = list(histories.keys())[0]\n",
        "    length = len(histories[first_key][0])\n",
        "    x_vals = np.linspace(1, length, length)\n",
        "\n",
        "    cmap = get_cmap(len(histories))\n",
        "    patches = []\n",
        "    for i, color in enumerate(cmap.colors):\n",
        "        patches.append(mpatches.Patch(color, color, label=f\"R = {list(histories.keys())[i]}\"))\n",
        "\n",
        "    plt.figure(figsize = (14, 7))\n",
        "\n",
        "    y_vals = np.mean(histories[first_key], 0)\n",
        "    if normalized:\n",
        "        y_vals = y_vals / y_vals[0]\n",
        "        plt.loglog(x_vals, y_vals, \n",
        "                 c=cmap(0), label='duality gap reduction in average')\n",
        "    else:\n",
        "        plt.loglog(x_vals, y_vals, \n",
        "                    c=cmap(0), label='duality gap reduction in average')\n",
        "\n",
        "    for i, key in enumerate(list(histories.keys())[1:]):\n",
        "\n",
        "        y_vals = np.mean(histories[key], 0)\n",
        "        if normalized:\n",
        "            y_vals = y_vals / y_vals[0]\n",
        "            plt.loglog(x_vals, y_vals, c=cmap(i+1))\n",
        "        else:\n",
        "            plt.loglog(x_vals, y_vals, c=cmap(i+1))\n",
        "        \n",
        "    plt.legend(loc='best', handles=patches, fontsize=17)\n",
        "    if normalized:\n",
        "        plt.title(f\"Normalized duality gap convergence in average for different R (loglog)\", fontsize=15)\n",
        "    else:\n",
        "        plt.title(f\"Duality gap convergence in average for different R (loglog)\", fontsize=15)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot3(histories, normalized=False):\n",
        "    \"\"\"\n",
        "    input: res[\"dg_histories\"]\n",
        "    \"\"\"\n",
        "    first_key = list(histories.keys())[0]\n",
        "    length = len(histories[first_key][0])\n",
        "    x_vals = np.linspace(1, length, length)\n",
        "\n",
        "    cmap = get_cmap(len(histories))\n",
        "    patches = []\n",
        "    for i, color in enumerate(cmap.colors):\n",
        "        patches.append(mpatches.Patch(color, color, label=f\"R = {list(histories.keys())[i]}\"))\n",
        "\n",
        "    plt.figure(figsize = (14, 7))\n",
        "\n",
        "    y_vals = np.mean(histories[first_key], 0)\n",
        "    std_vals = np.std(histories[first_key], 0)\n",
        "    if normalized:\n",
        "        std_vals = std_vals / y_vals[0]\n",
        "        y_vals = y_vals / y_vals[0]\n",
        "\n",
        "    plt.semilogx(x_vals, y_vals, \n",
        "                 c=cmap(0), label='duality gap reduction in average')\n",
        "    plt.fill_between(x_vals, \n",
        "                     y_vals - std_vals, \n",
        "                     y_vals + std_vals,\n",
        "                     color=cmap(0), alpha=0.2)\n",
        "\n",
        "    for i, key in enumerate(list(histories.keys())[1:]):\n",
        "\n",
        "        y_vals = np.mean(histories[key], 0)\n",
        "        std_vals = np.std(histories[key], 0)\n",
        "        if normalized:\n",
        "            std_vals = std_vals / y_vals[0]\n",
        "            y_vals = y_vals / y_vals[0]\n",
        "\n",
        "        plt.semilogx(x_vals, y_vals, c=cmap(i+1))\n",
        "        plt.fill_between(x_vals, \n",
        "                         y_vals - std_vals, \n",
        "                         y_vals + std_vals,\n",
        "                         color=cmap(i+1), alpha=0.2)\n",
        "        \n",
        "    plt.legend(loc='best', handles=patches, fontsize=17)\n",
        "    if normalized:\n",
        "        plt.title(f\"Normalized duality gap convergence in average with standard deviation for different R (semilogx)\", fontsize=15)\n",
        "    else:\n",
        "        plt.title(f\"Duality gap convergence in average with standard deviation for different R (semilogx)\", fontsize=15)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot4(all_res):\n",
        "    \"\"\"\n",
        "    input: all_res\n",
        "    \"\"\"\n",
        "    f_idx = 5001\n",
        "    first_key = list(all_res[\"fwm\"].keys())[0]\n",
        "    length = len(all_res[\"fwm\"][first_key][0])\n",
        "    x_vals = np.linspace(1, length, length)[:f_idx]\n",
        "\n",
        "    colors = [\"crimson\", \"teal\", \"orange\"]\n",
        "    patches = []\n",
        "    for i, color in enumerate(colors):\n",
        "        patches.append(mpatches.Patch(color, color, label=f\"{list(all_res.keys())[i]}\"))\n",
        "\n",
        "    plt.figure(figsize = (14, 7))\n",
        "\n",
        "    for i, key in enumerate(list(all_res.keys())):\n",
        "        for j, R in enumerate(list(all_res[key].keys())):\n",
        "            alphas = np.linspace(0.1, 1, len(all_res[key]))[::-1]\n",
        "            y = np.mean(all_res[key][R], 0)[:f_idx]\n",
        "            plt.loglog(x_vals, y/y[0], \n",
        "                        c=colors[i], alpha=alphas[::-1][j])\n",
        "        \n",
        "    plt.legend(loc='best', handles=patches, fontsize=17)\n",
        "    plt.title(\"Duality gap reduction methods comparison (fatter lines for bigger R)\", fontsize=15)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9jjlvB9mi-dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update libs"
      ],
      "metadata": {
        "id": "yks-HiNouBEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "importlib.reload(dh)\n",
        "importlib.reload(md)"
      ],
      "metadata": {
        "id": "cC2P1Apj9hXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`t_start` generation**\n",
        "\n",
        "  * $\\|{\\bf t^*}\\|_2 = \\sqrt{t_1^{*^2} + t_2^{*^2} + ... + t_n^{*^2}}$\n",
        "\n",
        "  * $\\|{\\bf t^*} - {\\bf t^0}\\|_2 = \\sqrt{(t_1^* - t^0_1)^2 + (t_2^* - t^0_2)^2 + ... + (t_n^* - t^0_n)^2} = R$\n",
        "\n",
        "  * $\\|{\\bf t^*} - {\\bf t^0}\\|_2 = \\sqrt{\\Delta_1^2 + \\Delta_2^2 + ... + \\Delta_n^2} = R$\n",
        "\n",
        "  * $\\Delta_1^2 + \\Delta_2^2 + ... + \\Delta_n^2 = R^2$\n",
        "\n",
        "  * $\\{\\xi_i\\}_{i=1}^n \\in N(0, 1)$\n",
        "\n",
        "  * ${\\bf\\Delta_k} = \\sqrt{\\dfrac{\\boldsymbol{\\xi}^2 \\cdot R^2}{\\sum_{i=1}^n\\xi_i^2}}$ - normalization\n",
        "  \n",
        "  * ${\\bf t^0_k} = {\\bf t^*} \\pm {\\bf\\Delta_k}$ - with probability $\\dfrac{1}{2}$"
      ],
      "metadata": {
        "id": "OAf6BU6_aXx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_name = 'Anaheim_net.tntp'\n",
        "trips_name = 'Anaheim_trips.tntp'\n",
        "\n",
        "handler = dh.DataHandler()\n",
        "graph_data = handler.GetGraphData(net_name, columns = ['init_node', 'term_node', 'capacity', 'free_flow_time'])\n",
        "graph_correspondences, total_od_flow = handler.GetGraphCorrespondences(trips_name)\n",
        "\n",
        "answer = handler.ReadAnswer('Anaheim_flow.tntp')\n",
        "\n",
        "print(f\"R for t_start = free_flow_times: {np.linalg.norm(answer['times'] - graph_data['graph_table']['free_flow_time'], 2)}\")"
      ],
      "metadata": {
        "id": "HYnOgq0Adgh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since for default experiment $R = \\|t^* - t^0\\|_2$ was equal to 3.2, we will consider starting points with R from 1 to 5."
      ],
      "metadata": {
        "id": "xjmFZJV4veG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_star = answer[\"times\"]\n",
        "\n",
        "def get_t0(R):\n",
        "    \"\"\"Given R generate start point from the sphere of radius R and center in t*\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    R: float value, R=||t_start - t*||_2\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    t0: start point\n",
        "    \"\"\"\n",
        "    xis = np.random.randn(len(t_star))\n",
        "    deltas = np.sqrt((xis**2 * np.float64(R**2)) / np.sum(xis**2))\n",
        "    if bernoulli.rvs(size=1, p=0.5)[0] == 0:\n",
        "        t0 = t_star - deltas\n",
        "    else:\n",
        "        t0 = t_star + deltas\n",
        "    return t0.clip(0)"
      ],
      "metadata": {
        "id": "NfsXMPqyHi5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment function"
      ],
      "metadata": {
        "id": "ktRf0kQYuSkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_results_to_json(res):\n",
        "    json_res = dict()\n",
        "    json_res[\"times\"] = res[\"times\"].tolist()\n",
        "    json_res[\"flows\"] = res[\"flows\"].tolist()\n",
        "    json_res[\"iter_num\"] = res[\"iter_num\"]\n",
        "    json_res[\"res_msg\"] = res[\"res_msg\"]\n",
        "    json_res[\"history\"] = res[\"history\"]\n",
        "    return json_res"
      ],
      "metadata": {
        "id": "uhroyWrfFCY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose several $R$, for each unique $R$ we generate random points from the sphere with radius $R$ and center in $t^*$. Run experiments with different starting points and collect histories (trajectories)."
      ],
      "metadata": {
        "id": "zYAWgURH-4Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_1(model, solver, solver_kwargs, R_list, nruns, save_path, composite=None):\n",
        "    \"\"\"We choose several 𝑅, for each unique 𝑅 we generate random points from the sphere \n",
        "    with radius 𝑅 and center in 𝑡∗ . Run experiments with different starting points and \n",
        "    collect histories (trajectories).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model:          md.Model() class instance\n",
        "    solver:         one of ['fwm', 'ustm', 'ugd']\n",
        "    solver_kwarg:   dict with kwargs\n",
        "    R_list:         list with different R\n",
        "    nruns:          number of runs (i.e. different starting points) for each R\n",
        "    save_path:      path for logs\n",
        "    composite:      for ustm and ugd\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    res:            logs for each run\n",
        "    \"\"\"\n",
        "    final_dg_means = []\n",
        "    final_dg_stds  = []\n",
        "    dg_histories   = dict()\n",
        "\n",
        "    # just in case\n",
        "    results = []\n",
        "\n",
        "    for j, R in enumerate(R_list):\n",
        "        print(f\"\\nRound: {j+1}/{len(R_list)}\")\n",
        "        final_dgs = []\n",
        "        dg_runs_histories = []\n",
        "        for i in range(nruns):\n",
        "            print(f\"Run: {i+1}/{nruns}\")\n",
        "            solver_kwargs[\"t_start\"] = get_t0(R)\n",
        "            res = model.find_equilibrium(solver_name=solver, composite=composite, solver_kwargs=solver_kwargs)\n",
        "            final_dgs.append(res[\"history\"][\"dual_gap\"][-1])\n",
        "            dg_runs_histories.append(res[\"history\"][\"dual_gap\"])\n",
        "\n",
        "            results.append(convert_results_to_json(res))\n",
        "            with open(save_path+solver+\"/exp1_results.json\", 'a') as f:\n",
        "                json.dump(convert_results_to_json(res), f)\n",
        "                f.write('\\n')\n",
        "        \n",
        "        with open(save_path+solver+\"/exp1_final_dg_means.json\", 'a') as f:\n",
        "            json.dump(np.mean(final_dgs).tolist(), f)\n",
        "            f.write('\\n')\n",
        "\n",
        "        with open(save_path+solver+\"/exp1_final_dg_stds.json\", 'a') as f:\n",
        "            json.dump(np.std(final_dgs).tolist(), f)\n",
        "            f.write('\\n')\n",
        "\n",
        "        with open(save_path+solver+\"/exp1_dg_histories.json\", 'a') as f:\n",
        "            json.dump({R: dg_runs_histories}, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "\n",
        "        final_dg_means.append(np.mean(final_dgs).tolist())\n",
        "        final_dg_stds.append(np.std(final_dgs).tolist())\n",
        "        dg_histories[R] = dg_runs_histories\n",
        "\n",
        "    res = dict()\n",
        "    res[\"final_dg_means\"] = final_dg_means\n",
        "    res[\"final_dg_stds\"]  = final_dg_stds\n",
        "    res[\"dg_histories\"]   = dg_histories\n",
        "    res[\"results\"]        = results\n",
        "\n",
        "    with open(save_path+solver+\"/exp1_all_results.json\", 'w') as f:\n",
        "        json.dump(res, f)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "ZhUL69e0DKYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frank-Wolfe method"
      ],
      "metadata": {
        "id": "o1klgf5tueJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment"
      ],
      "metadata": {
        "id": "3h_trJ9NWF5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beckmann_save = 'beckmann_results_anton/'\n",
        "solver = \"fwm\"\n",
        "MAX_ITER = 5000\n",
        "R_list = [1, 2, 3, 4, 5]\n",
        "nruns = 10"
      ],
      "metadata": {
        "id": "-L4CfAKqumca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = md.Model(graph_data, graph_correspondences, \n",
        "                    total_od_flow, mu = 0.25, rho = 0.15)\n",
        "\n",
        "solver_kwargs = {'max_iter' : MAX_ITER, 'stop_crit': 'max_iter',\n",
        "                 'verbose' : False, 'save_history' : True}\n",
        "\n",
        "res = run_experiment_1(model, solver, solver_kwargs, R_list, nruns, beckmann_save)"
      ],
      "metadata": {
        "id": "Cy05WNZzdkgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graphs"
      ],
      "metadata": {
        "id": "DHFYS0HvWH7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = {\"final_dg_means\": [], \"final_dg_stds\": []}\n",
        "\n",
        "with open(\"beckmann_results_anton/fwm/exp1_final_dg_means.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res1[\"final_dg_means\"].append(res)\n",
        "\n",
        "with open(\"beckmann_results_anton/fwm/exp1_final_dg_stds.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res1[\"final_dg_stds\"].append(res)"
      ],
      "metadata": {
        "id": "40U4Q7BJRv7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot1(res1[\"final_dg_means\"], res1[\"final_dg_stds\"])"
      ],
      "metadata": {
        "id": "VKKWKGwuR3Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = dict()\n",
        "with open(\"beckmann_results_anton/fwm/exp1_dg_histories.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res2[list(res.keys())[0]] = res[list(res.keys())[0]]"
      ],
      "metadata": {
        "id": "CSmek_GHR7_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot2(res2)"
      ],
      "metadata": {
        "id": "6KfzTiaZR9jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Universal Similar Triangles"
      ],
      "metadata": {
        "id": "nH2s0HmOtJ-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment"
      ],
      "metadata": {
        "id": "b-RJEgr6WLPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beckmann_save = 'beckmann_results_anton/'\n",
        "solver = \"ustm\"\n",
        "MAX_ITER = 5000\n",
        "R_list = [1, 2, 3, 4, 5]\n",
        "nruns = 10"
      ],
      "metadata": {
        "id": "G_vtX_EwtW25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = md.Model(graph_data, graph_correspondences, \n",
        "                    total_od_flow, mu = 0.25, rho = 0.15)\n",
        "\n",
        "solver_kwargs = {'eps_abs': None,\n",
        "                 'max_iter': MAX_ITER, 'stop_crit': 'max_iter',\n",
        "                 'verbose' : False, 'save_history' : True}\n",
        "\n",
        "res = run_experiment_1(model, solver, solver_kwargs, R_list, nruns, beckmann_save, composite=True)"
      ],
      "metadata": {
        "id": "vIpmSCiMkPpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graphs"
      ],
      "metadata": {
        "id": "jkbA7GxkWMt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = {\"final_dg_means\": [], \"final_dg_stds\": []}\n",
        "\n",
        "with open(\"beckmann_results_anton/ustm/exp1_final_dg_means.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res1[\"final_dg_means\"].append(res)\n",
        "\n",
        "with open(\"beckmann_results_anton/ustm/exp1_final_dg_stds.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res1[\"final_dg_stds\"].append(res)"
      ],
      "metadata": {
        "id": "9W4z1tObSc63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot1(res1[\"final_dg_means\"], res1[\"final_dg_stds\"])"
      ],
      "metadata": {
        "id": "irdIJIkxSeE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = dict()\n",
        "with open(\"beckmann_results_anton/ustm/exp1_dg_histories.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res2[list(res.keys())[0]] = res[list(res.keys())[0]]"
      ],
      "metadata": {
        "id": "c-s3uI-2Sq8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot2(res2)"
      ],
      "metadata": {
        "id": "hPTvt9ZeSst1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Universal Gradient Descent"
      ],
      "metadata": {
        "id": "0nQWmAu3vcW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment"
      ],
      "metadata": {
        "id": "YkRFl1h9Wdxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beckmann_save = 'beckmann_results_anton/'\n",
        "solver = \"ugd\"\n",
        "MAX_ITER = 10000\n",
        "R_list = [1, 2, 3, 4, 5]\n",
        "nruns = 10"
      ],
      "metadata": {
        "id": "7k8qhyPCvsod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = md.Model(graph_data, graph_correspondences, \n",
        "                    total_od_flow, mu = 0.25, rho = 0.15)\n",
        "\n",
        "solver_kwargs = {'eps_abs': None,\n",
        "                    'max_iter': MAX_ITER, 'stop_crit': 'max_iter',\n",
        "                    'verbose' : False, 'save_history' : True}\n",
        "\n",
        "res = run_experiment_1(model, solver, solver_kwargs, R_list, nruns, beckmann_save, composite=True)"
      ],
      "metadata": {
        "id": "KqtRZeqVvIEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graphs"
      ],
      "metadata": {
        "id": "TGoISFssWiOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = {\"final_dg_means\": [], \"final_dg_stds\": []}\n",
        "\n",
        "with open(\"beckmann_results_anton/ugd/exp1_final_dg_means.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res1[\"final_dg_means\"].append(res)\n",
        "\n",
        "with open(\"beckmann_results_anton/ugd/exp1_final_dg_stds.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res1[\"final_dg_stds\"].append(res)"
      ],
      "metadata": {
        "id": "h1EfeN7BNruh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot1(res1[\"final_dg_means\"], res1[\"final_dg_stds\"])"
      ],
      "metadata": {
        "id": "vpzbGDRywQh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = dict()\n",
        "with open(\"beckmann_results_anton/ugd/exp1_dg_histories.json\", \"r\") as f:\n",
        "    for line in f:\n",
        "        res = json.loads(line)\n",
        "        res2[list(res.keys())[0]] = res[list(res.keys())[0]]"
      ],
      "metadata": {
        "id": "XnBZSAsfY9KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot2(res2)"
      ],
      "metadata": {
        "id": "W_mWiboPwi4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot3(res2)"
      ],
      "metadata": {
        "id": "iY2BgjqXLay0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods comparison"
      ],
      "metadata": {
        "id": "BuOQ_--pxFaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_res = {\"fwm\": {}, \"ustm\": {}, \"ugd\": {}}\n",
        "for method in [\"fwm\", \"ustm\", \"ugd\"]:\n",
        "    with open(beckmann_save+method+\"/exp1_dg_histories.json\", \"r\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 4:\n",
        "                continue\n",
        "            res = json.loads(line)\n",
        "            all_res[method][list(res.keys())[0]] = res[list(res.keys())[0]]"
      ],
      "metadata": {
        "id": "PLQz-iVO2VIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot4(all_res)"
      ],
      "metadata": {
        "id": "cDp7gBeG3Dz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "s8RHV1iHCGWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Beckman model 3 methods were researched for duality gap reduction in **average**. As a result, Frank-Wolfe method seems to be stable for choosing starting point in average, Universal Gradient Descent suffer from far from solution starting points and Universal Similar Triangles method showed phenomenal results: for more distant from solution starting points it showed better result. Phenomenon could be explained by averaging of trajectories and lack of iterations."
      ],
      "metadata": {
        "id": "c5c-3mvSCJG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jeXoC_dICIQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}